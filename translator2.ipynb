{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOa/EZFD7vDJrMohblWPosv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eissana/translator/blob/master/translator2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchtext==0.6"
      ],
      "metadata": {
        "id": "tHgQZOIQhTks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade spacy"
      ],
      "metadata": {
        "id": "y03iOMhOhU5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "efAxiX6lgzeR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(logits, y, ignore_index):\n",
        "    \"\"\"\n",
        "    Computes cross-entropy loss, given logits and labels.\n",
        "    \"\"\"\n",
        "    B, T, C = logits.shape\n",
        "    # F.cross_entropy expects size C, (B, C), or (B, C, ...)\n",
        "    # logits shape is (B, T, C), so we flatten the first two dimensions.\n",
        "    return F.cross_entropy(\n",
        "        logits.view(B * T, C), y.reshape(B * T), ignore_index=ignore_index\n",
        "    )\n",
        "    # loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_pad_index)\n",
        "    # return loss_fn(logits.view(B*T, C), y.reshape(B*T))\n",
        "\n",
        "\n",
        "def text2tokens(text, tokenizer):\n",
        "    tokens = [Preprocessor.INIT_TOKEN]\n",
        "    tokens.extend([t.text.lower() for t in tokenizer(text)])\n",
        "    tokens.append(Preprocessor.EOS_TOKEN)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def src2target(tokens, model, preprocessor, block_size, device):\n",
        "    \"\"\"\n",
        "    Gets source language tokens, calls model to translate them, and returns\n",
        "    target tokens.\n",
        "    \"\"\"\n",
        "    token_ids = [preprocessor.src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    x = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    sos = preprocessor.tgt_field.vocab.stoi[Preprocessor.INIT_TOKEN]\n",
        "    eos = preprocessor.tgt_field.vocab.stoi[Preprocessor.EOS_TOKEN]\n",
        "\n",
        "    y = torch.tensor([[sos]], dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(block_size):\n",
        "        with torch.no_grad():\n",
        "            logits = model(x, y)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        scores = F.softmax(logits, dim=-1)\n",
        "        next_token = scores.multinomial(1)\n",
        "        # next_token = logits.argmax(dim=-1).unsqueeze(0)\n",
        "\n",
        "        if next_token.item() == eos:\n",
        "            break\n",
        "\n",
        "        y = torch.cat((y, next_token), dim=-1)\n",
        "\n",
        "    y = y.view(-1)\n",
        "    y = [preprocessor.tgt_field.vocab.itos[t] for t in y]\n",
        "\n",
        "    return y[1:]\n",
        "\n",
        "\n",
        "def translate(text, model, preprocessor, block_size, device):\n",
        "    src_tokens = text2tokens(text, preprocessor.src_spacy.tokenizer)\n",
        "    tgt_tokens = src2target(src_tokens, model, preprocessor, block_size, device)\n",
        "    return \" \".join(tgt_tokens)\n",
        "\n",
        "\n",
        "def bleu(data, model, preprocessor, block_size, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = example.src\n",
        "        trg = example.trg\n",
        "        prediction = src2target(src, model, preprocessor, block_size, device)\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n"
      ],
      "metadata": {
        "id": "O-PFAf6Wh-Gi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "s5iyGpM8jl9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%python3 -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "ThsnOhMojnis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/eissana/translator/master/multi30k.sh\n",
        "!sh multi30k.sh"
      ],
      "metadata": {
        "id": "2XmKfU1PkAsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor():\n",
        "    INIT_TOKEN = \"<init>\"\n",
        "    EOS_TOKEN = \"<eos>\"  # end of sentence\n",
        "\n",
        "    def __init__(self, spacy_names, exts, data_root, min_freq, max_size):\n",
        "        self.src_spacy = spacy.load(spacy_names[0])\n",
        "        self.tgt_spacy = spacy.load(spacy_names[1])\n",
        "\n",
        "        def src_tokenize(text):\n",
        "            return [t.text for t in self.src_spacy.tokenizer(text)]\n",
        "\n",
        "        def tgt_tokenize(text):\n",
        "            return [t.text for t in self.tgt_spacy.tokenizer(text)]\n",
        "\n",
        "        self.src_field = Field(\n",
        "            tokenize=src_tokenize,\n",
        "            init_token=self.INIT_TOKEN,\n",
        "            eos_token=self.EOS_TOKEN,\n",
        "            lower=True,\n",
        "        )\n",
        "        self.tgt_field = Field(\n",
        "            tokenize=tgt_tokenize,\n",
        "            init_token=self.INIT_TOKEN,\n",
        "            eos_token=self.EOS_TOKEN,\n",
        "            lower=True,\n",
        "        )\n",
        "\n",
        "        # Run the following command to download data:\n",
        "        # > sh multi30k.sh\n",
        "        self.train, self.val, self.test = Multi30k.splits(\n",
        "            exts=exts,\n",
        "            fields=(self.src_field, self.tgt_field),\n",
        "            root=data_root,  # data/multi30k/\n",
        "        )\n",
        "        self.src_field.build_vocab(self.train, max_size=max_size, min_freq=min_freq)\n",
        "        self.tgt_field.build_vocab(self.train, max_size=max_size, min_freq=min_freq)\n"
      ],
      "metadata": {
        "id": "X64qkAaph4V_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"running on {device}\")\n",
        "model_filename = \"models/model.pt\"\n",
        "\n",
        "example_text = \"Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\"\n",
        "\n",
        "load_model = True\n",
        "save_model = True\n",
        "\n",
        "params = {\n",
        "    \"epochs\": 10,\n",
        "    \"learning_rate\": 3.0e-4,\n",
        "    \"batch_size\": 32,\n",
        "    \"embedding_dim\": 512,\n",
        "    \"nhead\": 8,\n",
        "    \"num_encoder_layers\": 3,\n",
        "    \"num_decoder_layers\": 3,\n",
        "    \"dropout\": 0.1,\n",
        "    \"block_size\": 100,\n",
        "    \"dim_feedforward\": 4,\n",
        "}\n",
        "\n",
        "losses = {\n",
        "    \"train\": [],\n",
        "    \"val\": [],\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyBKOEFAnYQO",
        "outputId": "5759a174-853e-42fa-f6b5-b4f3cbcf7d70"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    '''\n",
        "    Self-attention head layer.\n",
        "    '''\n",
        "    def __init__(self, head_size, params, use_mask):\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "        block_size = params['block_size']\n",
        "\n",
        "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(params['dropout'])\n",
        "\n",
        "        self.use_mask = use_mask\n",
        "        if use_mask:\n",
        "          # tril is not a model parameter so we register it as a buffer.\n",
        "          # block_size is the maximum size. The actual size can be smaller.\n",
        "          self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, v, k, q):\n",
        "        _, T, C = q.shape\n",
        "        value, key, query = self.value(v), self.key(k), self.query(q)\n",
        "        weights = query @ key.transpose(-2, -1) * C**-0.5\n",
        "\n",
        "        if self.use_mask:\n",
        "          # The time dimension can be smaller than the block-size.\n",
        "          weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        out = weights @ value\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self, params, use_mask, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "        nhead = params['nhead']\n",
        "        assert embedding_dim % nhead == 0, f\"{embedding_dim=} must be divisible by {nhead=}\"\n",
        "        head_size = embedding_dim // nhead\n",
        "\n",
        "        self.ln = nn.LayerNorm(embedding_dim)\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(head_size, params, use_mask) for _ in range(nhead)])\n",
        "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.dropout = nn.Dropout(params['dropout'])\n",
        "\n",
        "    def forward(self, v, k, q):\n",
        "        v, k, q = self.ln(v), self.ln(k), self.ln(q)\n",
        "        out = torch.cat([head(v, k, q) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "        dim_feedforward = params['dim_feedforward']\n",
        "\n",
        "        # feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.LayerNorm(embedding_dim),\n",
        "            nn.Linear(embedding_dim, dim_feedforward * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward * embedding_dim, embedding_dim),  # projection\n",
        "            nn.Dropout(params['dropout'])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ffn(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, params, device):\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "        nhead = params['nhead']\n",
        "\n",
        "        # multi-head self attention with no mask. All nodes are allowed to\n",
        "        # communicate freely.\n",
        "        self.attn = MultiHead(params, use_mask=False, device=device)\n",
        "        self.ffn = FeedForward(params)\n",
        "\n",
        "    def forward(self, v, k, q):\n",
        "        out = q + self.attn(v, k, q)\n",
        "        out = out + self.ffn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, params, device):\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "\n",
        "        # multi-head self attention with triangular mask. Nodes communicate only\n",
        "        # with previous nodes.\n",
        "        self.attn = MultiHead(params, use_mask=True, device=device)\n",
        "        # Reusing Encoder as the top part of the decoder with a multi-head\n",
        "        # cross-attention and a feed-forward network on top of it.\n",
        "        self.attn_ffn = EncoderBlock(params, device)\n",
        "\n",
        "    def forward(self, enc_out, dec_in):\n",
        "        out = dec_in\n",
        "        out = out + self.attn(out, out, out)\n",
        "        out = out + self.attn_ffn(enc_out, enc_out, out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, device, params):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        embedding_dim = params['embedding_dim']\n",
        "        block_size = params['block_size']\n",
        "        num_encoder_layers = params['num_encoder_layers']\n",
        "        num_decoder_layers = params['num_decoder_layers']\n",
        "        dropout = params['dropout']\n",
        "\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, embedding_dim)\n",
        "        self.src_pos = nn.Embedding(block_size, embedding_dim)\n",
        "\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
        "        self.tgt_pos = nn.Embedding(block_size, embedding_dim)\n",
        "\n",
        "        self.encoders = nn.ModuleList(\n",
        "            [EncoderBlock(params, device) for _ in range(num_encoder_layers)])\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [DecoderBlock(params, device) for _ in range(num_decoder_layers)])\n",
        "\n",
        "        self.proj = nn.Linear(embedding_dim, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        _, srcT = src.shape\n",
        "        src_positions = torch.arange(srcT).unsqueeze(0).to(self.device)\n",
        "        src_out = self.src_emb(src) + self.src_pos(src_positions)\n",
        "        src_out = self.dropout(src_out)\n",
        "\n",
        "        _, tgtT = tgt.shape\n",
        "        tgt_positions = torch.arange(tgtT).unsqueeze(0).to(self.device)\n",
        "        tgt_out = self.tgt_emb(tgt) + self.tgt_pos(tgt_positions)\n",
        "        tgt_out = self.dropout(tgt_out)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            src_out = encoder(src_out, src_out, src_out)\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            tgt_out = decoder(src_out, tgt_out)\n",
        "\n",
        "        tgt_out = self.proj(tgt_out)\n",
        "        tgt_out = self.dropout(tgt_out)\n",
        "\n",
        "        return tgt_out\n"
      ],
      "metadata": {
        "id": "jqUOrtbBkpTK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pp = Preprocessor(\n",
        "    spacy_names=(\"de_core_news_sm\", \"en_core_web_sm\"),\n",
        "    exts=(\".de\", \".en\"),\n",
        "    data_root=\"data\",\n",
        "    min_freq=2,\n",
        "    max_size=10000,\n",
        ")\n",
        "\n",
        "src_vocab_size = len(pp.src_field.vocab)\n",
        "tgt_vocab_size = len(pp.tgt_field.vocab)\n",
        "\n",
        "train_iter, val_iter, _ = BucketIterator.splits(\n",
        "    datasets=(pp.train, pp.val, pp.test),\n",
        "    batch_size=params[\"batch_size\"],\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    device=device,\n",
        "    params=params,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=10\n",
        ")\n",
        "\n",
        "if load_model:\n",
        "    state = torch.load(model_filename)\n",
        "\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    optimizer.load_state_dict(state[\"optimizer\"])\n",
        "    scheduler.load_state_dict(state[\"scheduler\"])\n",
        "\n",
        "num_params = sum([p.nelement() for p in model.parameters()])\n",
        "print(f\"\\nmodel parameters: {num_params}\")\n",
        "print(f\"\\n{params=}\")\n",
        "\n",
        "print(f\"\\nexample text to trasnlate: {example_text}\")\n",
        "\n",
        "answer = input(\"\\nwould you like to proceed to training? (y/n): \")\n",
        "if answer.lower() in {\"y\", \"yes\"}:\n",
        "    for epoch in range(params[\"epochs\"]):\n",
        "        print(f\"epoch {epoch} / {params['epochs']}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          translated_text = translate(\n",
        "              example_text,\n",
        "              model,\n",
        "              pp,\n",
        "              params[\"block_size\"],\n",
        "              device,\n",
        "          )\n",
        "          print(f\"translated example text:\\n{translated_text}\")\n",
        "\n",
        "          batch_loss = []\n",
        "          for batch in val_iter:\n",
        "              src = batch.src.T.to(device)\n",
        "              tgt = batch.trg.T.to(device)\n",
        "\n",
        "              logits = model(src, tgt[:, :-1])\n",
        "              vloss = get_loss(\n",
        "                  logits,\n",
        "                  tgt[:, 1:],\n",
        "                  ignore_index=pp.tgt_field.vocab.stoi[pp.tgt_field.pad_token],\n",
        "                )\n",
        "              batch_loss.append(vloss.item())\n",
        "\n",
        "          losses['val'].extend(batch_loss)\n",
        "\n",
        "        model.train()\n",
        "        batch_loss = []\n",
        "        for batch in train_iter:\n",
        "            src = batch.src.T.to(device)\n",
        "            tgt = batch.trg.T.to(device)\n",
        "\n",
        "            logits = model(src, tgt[:, :-1])\n",
        "            loss = get_loss(\n",
        "                logits,\n",
        "                tgt[:, 1:],\n",
        "                ignore_index=pp.tgt_field.vocab.stoi[pp.tgt_field.pad_token],\n",
        "            )\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "        losses['train'].extend(batch_loss)\n",
        "        if save_model:\n",
        "            checkpoint = {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "            }\n",
        "            torch.save(checkpoint, model_filename)\n",
        "\n",
        "        scheduler.step(np.mean(batch_loss))\n",
        "\n",
        "print(f\"\\ncomputing bleu score...\")\n",
        "score = bleu(pp.test[:100], model, pp, params[\"block_size\"], device) * 100\n",
        "print(f\"bleu score: {score:0.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veu-pAsniL5_",
        "outputId": "a62561ff-0801-410c-bd77-8879ffd4df37"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model parameters: 32218885\n",
            "\n",
            "params={'epochs': 10, 'learning_rate': 0.0003, 'batch_size': 32, 'embedding_dim': 512, 'nhead': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.1, 'block_size': 100, 'dim_feedforward': 4}\n",
            "\n",
            "example text to trasnlate: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "epoch 0 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 1 / 10\n",
            "translated example text:\n",
            "two young , white men are outside near many bushes .\n",
            "epoch 2 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 3 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 4 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 5 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 6 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 7 / 10\n",
            "translated example text:\n",
            "two young men outside near many bushes .\n",
            "epoch 8 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "epoch 9 / 10\n",
            "translated example text:\n",
            "two young , white males are outside near many bushes .\n",
            "\n",
            "computing bleu score...\n",
            "bleu score: 14.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"loss of a random model: {np.log(len(pp.tgt_field.vocab))}\")\n",
        "print(f\"final training loss: {np.mean(losses['train'])}\")\n",
        "print(f\"final validation loss: {np.mean(losses['val'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsasU9U__npN",
        "outputId": "89579ccd-5a7d-4d1f-f77a-59821c5b2478"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss of a random model: 8.681520484837913\n",
            "final training loss: 1.1286912155497473\n",
            "final validation loss: 3.281958619132638\n"
          ]
        }
      ]
    }
  ]
}